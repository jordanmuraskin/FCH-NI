\section*{Methods}
\subsection*{Subjects}

21 subjects (12 male, 9 female; age range 20-35 years) participated in the study. The Columbia University Institutional Review Board (IRB) approved all experiments and informed consent was obtained before the start of each experiment. All subjects had normal or corrected-to-normal vision.

\subsection*{Stimuli}
We used a set of 30 face (from the Max Planck Institute face database), 30 car, and 30 house (obtained from the web) gray scale images (image size 512x512 pixels, 8 bits/pixel). They all had identical magnitude spectra (average magnitude spectrum of all images in the database) and their corresponding phase spectra were manipulated using the weighted mean phase (WMP) \cite{Dakin2002} technique to generate a set of images characterized by their \% phase coherence. The stimulus evidence (high or low) for each trial was systematically varied by modifying the salience of the image via randomization of image phase at either 35\% (low) or  50\% (high) coherence.

\subsection*{Experimental task}
The stimuli were used in an event-related three-alternative forced choice (3-AFC) visual discrimination task. On each trial, an image — either a face, car, or house — was presented for 50 ms and subjects were instructed to respond with the category of the image by pressing one of three buttons on an MR compatible button controller. Stimuli were presented to subjects using E-Prime software (Psychology Software Tools) and a VisuaStim Digital System (Resonance Technology) with 600x800 goggle display. Images  subtended 11\textdegree x 8\textdegree of visual angle. Over four runs, a total of 720 trials were acquired (240 of each category with 120 high coherence trials) with a random inter-trial interval (ITI) sampled uniformly between 2-4s. Each run lasted for 560 seconds. 

\subsection*{fMRI acquisition}
Blood-oxygenation-level-dependent (BOLD) T2*-weighted functional images were acquired on a 3T Philips Achieva scanner using a gradient-echo echo-planar imaging (EPI) pulse sequence with the following parameters: Repetition time (TR) 2000ms, echo time (TE) 25ms, flip angle 90°, slice thickness 3mm, interslice gap 1mm, in-plane resolution 3x3mm, 27 slices per volume, 280 volumes. For all of the participants, we also acquired a standard T1-weighted structural MRI scan (SPGR, resolution 1x1x1mm). 

\subsection*{EEG acquisition}
We simultaneously and continuously recorded EEG using a custom-built MR-compatible EEG system \cite{Sajda2007,Sajda2010a}, with differential amplifiers and bipolar EEG montage. The caps were configured with 36 Ag/AgCl electrodes including left and right mastoids, arranged as 43 bipolar pairs. Further details are given in Supplementary Material. 

\subsection*{Functional image pre-processing}
Image preprocessing was performed with FSL (www.fmrib.ox.ac.uk/fsl/). Functional images were spatially realigned to the middle image in the times series (motion-correction), corrected for slice time acquisition, spatially smoothed with a 6mm FWHM Gaussian kernel, and high pass filtered (100s). The structural images were segmented (into grey matter, white matter and cerebro-spinal fluid), bias corrected and spatially normalized to the MNI template using ‘FAST’ \cite{Zhang2001a}. Functional images were registered into MNI space using boundary based registration (BBR) \cite{Greve2009}.

\subsection*{EEG data preprocessing}
We performed standard EEG preprocessing offline using MATLAB (MathWorks) with the following digital Butterworth filters: 0.5 Hz high pass to remove direct current drift, 60 and 120 Hz notches to remove electrical line noise and its first harmonic, and 100 Hz low pass to remove high-frequency artifacts not associated with neurophysiological processes. These filters were applied together in the form of a zero-phase finite impulse response filter to avoid distortions caused by phase delays. We extracted stimulus-locked 1500 ms epochs (-500:1000) and subtracted the mean baseline (-200:0) from the rest of the epoch. Through visual inspection, we discarded trials containing motion and/or blink artifacts, evidenced by sudden high-amplitude deflections.

\subsection*{Estimating EEG components and trial-to-trial variability}
We used logistic regression to associate each trial with the level of stimulus evidence represented in the EEG. We considered high stimulus and low stimulus evidence trials irrespective of behavioral accuracy. Regularized logistic regression (r-LR) was used as a classifier to find an optimal projection for discriminating between high and low stimulus evidence trials over a specific temporal window. A sweep of the regularization parameters was implemented using FaSTGLZ \cite{Conroy2013a}. This approach has been previously applied to identify neural components underlying rapid perceptual decision-making \cite{Goldman2009,Muraskin2015,Parra2005,Philiastides2014,Sherwin2012,Sherwin2013,Walz2013,Walz2013a}. 

Specifically, we defined 50ms duration training windows centered at time, $\tau$, ranging from stimulus onset to 800ms following the stimulus in 25ms steps. We used r-LR to estimate a spatial weighting, on all the N EEG channels, that discriminated high stimulus evidence from low stimulus evidence trials. This identifies an Nx1 vector, $w_{\tau}$ , which defines a hyperplane that maximally discriminates between each class (e.g., high vs. low stimulus evidence trials) in the N dimensional space of electrodes. Using this $w_{\tau}$ we calculate the distance each trial is from the decision boundary

\begin{equation}
    y_{\tau}=w^{T}_{\tau}E_{\tau}
\end{equation}

In eqn. 1, $E_{\tau}$ is an N x p vector (N sensors per time window, $\tau$, by p trials). We varied the center of the window ($\tau$) across the trial in 25ms time-steps. For each subject, this produced a matrix $Y$ where the rows corresponded to trials and the columns to training windows, i.e.  $Y$ incorporates the time-window localized trial-to-trial variability in the EEG that we subsequently use to tag the BOLD data. We quantified the performance of the linear discriminator by the area under the receiver operator characteristic (ROC) curve, referred to here as AUC, using a leave-one-out procedure. We used the ROC AUC metric to characterize the discrimination performance at each $\tau$. 

\subsection*{Traditional fMRI analysis}
We first ran a traditional general linear model (GLM) fMRI analysis in FSL, using event-related (high and low stimulus evidence) and response time (RT) variability regressors. Activated regions that passed a family-wise error (FWE) \cite{Nichols2003} corrected cluster threshold of $p < 0.01$ at a z-score threshold of 2.57 were considered significant. Further details can be found in the Supplemental Material. 

\subsection*{fMRI deconvolution}
Associating fMRI data to each trial is challenging for two main reasons: (a) the temporal dynamics of the hemodynamic response function (HRF) evolve over a longer time-scale than the mean ITI of the event-related design, resulting in overlapping responses between adjacent trials; and (b) the ITI was random for each trial so that the fMRI data was not acquired at a common lag relative to stimulus onset. To overcome these issues, we employed the `least squares - separate' (LS-S) deconvolution \cite{Mumford2012} method to estimate the voxel activations for each trial. We extracted 58697 voxels from a common gray matter group mask at $3mm^{3}$ spatial resolution that excluded white matter and CSF and assembled the resulting voxel activations into rows of the data matrix $F$.

\subsection*{Single subject encoding model}
All encoding model analyses were performed in MATLAB. To relate the EEG data with the fMRI, we devised a subject-wise spatiotemporal decomposition using singular value decomposition (SVD). Let F be an m x p matrix denoting m-voxels and p-trials that is the deconvolved high and low stimulus evidence fMRI data for each trial. Let Y be the r x p matrix denoting r-windows (33 $EEG_{\tau}$ windows and response time (RT)) and p-trials. For each trial, the first row of $Y$ is the response times while subsequent rows are the y values at each window time. Let W be an m x r matrix that is the weights on $Y$ that solve for $F$.

\begin{equation}
    F=WY
\end{equation}

Normally, if we solve for $W$ using the least squares approach, we obtain:
\begin{equation}
    W=(FY^{T})(YY^{T})^{-1}	
\end{equation}					

However, each time point might be highly correlated with its neighbors, which reduces the stability of the least-squares regression. We use SVD to reduce the feature space and improve our estimation of $W$ (the weights on each window). For a leave-one-out cross validation, we hold out a single trial from the EEG $Y$ matrix and the corresponding volume from the fMRI data $F$ and train on the remaining trials. We repeated this for all trials. For each leave one out fold we calculated,
\begin{equation}
    Y^{Train}=U \Sigma V^{T}
\end{equation}					
where $U$ is an r x r orthonormal matrix, $\Sigma$ is a r x p diagonal matrix and $V$ is a p x p orthonormal matrix. After SVD on $Y^{Train}$, we reduced the feature dimensions on $Y^{Train}$  to retain 75\% of the variance by only keeping v components. To do this, we selected the first v rows of $\Sigma$ and zeroed the other rows. This results in $\tilde{\Sigma}$  which is our reduced subspace matrix. We recalculate our least squares solution where we have replaced $Y$ by its reduced form $U\tilde{\Sigma}V^{T}$ in equation 3:
 		
\begin{equation}
    \hat{W} = (F^{Train}V\tilde{\Sigma}^{T})(\Sigma\Sigma^{T})^{-1}U^{T}
\end{equation} 				
 			
For each leave one out fold, we first calculated the SVD of the training set. We then calculated the number of components to keep and then solve for $\hat{W}$ , the weight estimate per fold. To test, we applied the weights to the left-out test data $Y^{Test}$ to estimate the encoded fMRI data $\hat{F}$ for the encoding part:

\begin{equation}
    \hat{F}=\hat{W}Y^{Test}
\end{equation}
We constructed the m x r weight matrix $W$ by taking the average of all the trained $\bar{W}$ matrices. For the decoding model we use the left out test data $F^{Test}$ and estimate the EEG variability $\hat{Y}$:
 \begin{equation}
     \hat{Y} = \hat{W}^{T}F^{Test}(\hat{W}^{T}\hat{W})^{+}
 \end{equation}
 Here, $\hat{W}^{T}\hat{W}$ is not invertible, and instead use the pseudo-inverse (denoted by +).
 
Given  $\hat{F}$, an m x p matrix with m voxels by p trials, for each voxel j, we calculated the correlation over trials of $\hat{F_{j}}$ with $F^{Test}_{j}$, resulting in the matrices $R^{fMRI}$ (Pearson Correlation Map) and $P^{fMRI}$  (p-value map of the Pearson Correlation) that are m x 1. The $P^{fMRI}$ was then converted to a z-score map. Thus, we have a map for each subject indicating that subject’s encoding model significance at each voxel. To test which time windows were significant in the decoding model, we also calculated,  $R^{EEG}_{\tau}$ , the correlation between $\hat{Y_{\tau}}$ and $Y_{\tau}$. For each subject, $R^{EEG}_{\tau}$ indicates how well the decoding model predicts EEG variability at each time window from a single fMRI volume. 

% It is important to note that this approach does not attempt to improve source localization typically done for EEG/MEG studies. Our approach instead provides the temporal resolution of EEG (ms) and the spatial resolution of fMRI (mm) without the need to solve the ill-posed inverse solution and make the many associated assumptions required for reliable source-localization results \cite{Wendel2009}.

\subsection*{Group level spatiotemporal analysis}

We evaluated both the encoding model and the resulting decoding model at the group level.  First, to identify group level voxels where our encoding model predictions were consistent across subjects, each subject's p-value maps for the leave-one-out correlation ($R^{fMRI}$) were converted into their respective z-values, then a one-sample t-test testing for the mean difference from 0 was calculated for each voxel and voxel-wise significance was calculated using threshold-free cluster enhancement (TFCE) using a non-parametric randomization procedure implemented in FSL\cite{Smith2009a}. TFCE is an alternative to cluster thresholding in which raw voxel-wise t-statistics are adjusted based on neighboring spatial clusters, producing corrected whole brain volumes. It generally allows for better sensitivity than other thresholding methods. Voxels were considered significant if they passed a false discovery rate (FDR) threshold of p \textless 0.01. 

These significant voxels were then used as a mask to temporally localize weight values in $\hat{W}$  by computing the voxels that were consistent in their direction (positive (high stimulus evidence) or negative (low stimulus evidence)) and timing ($\tau$ window) across subjects. Here, the goal was to develop a statistical parametric map of $\hat{W}$ that identifies significant results in both space and time, thereby creating a spatiotemporal map of a perceptual decision.

First, a one-sample t-test across the subjects’ $\hat{W}$  was computed for all voxels. This generates a statistical parametric map of T-scores in space (voxels) and time (windows). To correct for multiple comparisons, we implemented a spatiotemporal TFCE (stTFCE) in both space (neighboring voxels) and time (neighboring time windows - response time window not included) and computed significance through a randomization procedure. 33000 permutations (1000 permutations per window) were run by randomly altering the sign of the voxel weights from each subject and the temporal ordering of the windows, as we were testing whether the weights were consistent in sign, voxel space, and temporal window. The final statistical parametric map was calculated by comparing the true stTFCE value with the distribution of permuted values for each voxel. Again, voxels were considered significant if they passed FDR correction at p \textless 0.05 (high stimulus evidence: FDR-Corrected p \textless 0.0019, low stimulus evidence: FDR-Corrected p \textless 0.00036). Note that now our number of multiple comparisons was the number of voxels in the FDR-mask (20256) times the number of time windows (33). We analyzed the response time separately with a standard TFCE randomization procedure implemented in FSL.

We also tested the quality of the resulting decoding model at the group level.  To construct group level statistics for the decoding model, we first analyzed the $R^{EEG}_{\tau}$ vectors across all subjects to identify time windows when the model was consistent across subjects. The vectors were converted into their p-values, and for each time window ($\tau$), were used to compute combined Stouffer p-values 45 across all subjects\cite{Darlington2000}. These group level results were then corrected for multiple comparisons using p<0.05 FDR. 

Next, to identify group level voxels where our encoding model predictions were consistent across subjects, each subject's p-value maps for the leave-one-out correlation were converted into their respective z-values, and voxel-wise significance was calculated using threshold-free cluster enhancement (TFCE) using a non-parametric randomization procedure implemented in FSL \cite{Smith2009a}. Voxels were considered significant if they passed a false discovery rate threshold of $p<0.01$. 

These significant voxels were then used as a mask to temporally localize weight values in $\bar{W}$  by computing the voxels that were consistent in their direction (positive (high stimulus evidence) or negative (low stimulus evidence)) and timing ($\tau$ window). To this end, we implemented a spatiotemporal TFCE (stTFCE) in both space (neighboring voxels) and time (neighboring time windows - response time window not included) and computed significance through a randomization procedure. 33000 permutations (1000 permutations per window) were run by randomly altering the sign of the voxel weights from each subject and the temporal ordering of the windows, as we were testing whether the weights were consistent in sign, voxel space, and temporal window. P-values were calculated by comparing the true stTFCE value with the distribution of permuted values. Again, voxels were considered significant if they passed FDR correction at $p<0.05$ (high stimulus evidence: FDR-Corrected $p<0.0019$, low stimulus evidence: FDR-Corrected $p<0.00036$). Note, that now our number of multiple comparisons was the number of voxels in the FDR-mask (20256) times the number of time windows (33). We analyzed the response time separately with a standard TFCE randomization procedure implemented in FSL.

\subsection*{Drift diffusion model (DDM) and confidence proxy}
To analyze and better interpret our novel finding of a reactivation in the neural cascade (see Results), we used a DDM to estimate a proxy of confidence in the decision for each trial. The DDM models decision-making in two-choice tasks. Here, we treated the decision (correct vs. incorrect) as our two choices. A drift-process accumulates evidence over time until it crosses one of two boundaries (upper or lower) and initiates the corresponding response\cite{Ratcliff2008}. The speed with which the accumulation process approaches one of the two boundaries (a) is called drift-rate (v) and represents the relative evidence for or against a particular response. 

We used Hierarchical Bayesian estimation of the Drift-Diffusion Model in Python (HDDM) to calculate the drift rate (v), decision boundary (a) and non-decision time $T_{non}$ for each subject \cite{Wiecki2013}. Specifically, we modeled high and low stimulus evidence response time data separately. This was to ensure our confidence proxies were consistent within trial types. We included the response time and whether the subject got the trial correct. HDDM obtains a sequence of samples (i.e., a Markov chain Monte Carlo; MCMC) from the posterior of each parameter in the DDM. In our model, we generated 5000 samples from the posteriors, the first 1000 (burn-in) samples were discarded, and the remaining samples were thinned by 5\%.

Recently, Philiastides et al. \cite{Philiastides2014} showed that for conditions in which the boundary does not change, a proxy for decision confidence for each trial (i) can be computed by calculating $1/\sqrt{RT_{i}-T_{non}}$ , where $T_{non}$ is a subject dependent non-decision time fit by the model. Thus, after modeling the DDM process, each trial's (i) confidence proxy (CP) for each subject (j) was computed by $CP_{i,j} = 1/ \sqrt{RT_{i}-T_{non,j}}$  and then z-scored across trials where $T_{non,j}$ was varied for high or low stimulus evidence trials, separately, so that CP was a measure of relative trial confidence within difficulty levels. 

Using the encoding model weights ($\bar{W}$), we are able to estimate the trial-to-trial activation amplitude at different time windows. To estimate the reactivation amplitudes, we took the mean across time of $Y^{R}_{j,i} = W^{T}_{j,PostACC}F_{j,i}$  for each subject (j) and trial (i). Here,$W_{PostACC}$ is the weight matrix from the encoding model restricted to voxels that were significant in the spatiotemporal group results from the 600-800ms windows. By applying the weights restricted to these spatial areas and temporal windows to each trial, we are able to get a trial-to-trial value for ‘reactivation’. Note that because the reactivation is stronger in the more difficult trials the significant voxels in W are negative and so more negative y's indicate stronger reactivation while more positive y's indicate weaker reactivation. $Y^{R}_{j,i}$ was quintiled for high and low stimulus evidence and the average confidence proxy was calculated within each quintile. A linear mixed effects model \cite{Bates2015} was used to test if the slope of confidences across quintile grouping, $Y^{R}_{j,i}$ , was significantly different from 0 while including stimulus evidence as a condition. Separate similar analyses with non-replay windows (175-250ms) and testing for behavioral accuracy were also performed. To test the contribution of each cluster to the correlation with confidence, we implemented recursive feature elimination, where our features were clusters of significant voxels ($> 48$ voxels) during the 600-800ms time window. This procedure removed clusters from the ‘reactivation’ network before calculating trial-to-trial reactivation. We then calculated the percent change in slope (reactivation x confidence proxy) when the cluster was removed compared to the total network. This procedure ranks cluster importance by sorting which clusters, when removed, had the strongest negative effect on slope height.
